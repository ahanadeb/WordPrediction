{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 195133 chars, 94 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('Manifesto.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) \n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'q': 0, '(': 1, 'D': 2, 'ô': 3, 'P': 4, 'ó': 5, 'é': 6, '5': 7, '9': 8, 'l': 9, 'Y': 10, 'p': 11, 'o': 12, 'è': 13, 'z': 14, 'ä': 15, ']': 16, '?': 17, '\\n': 18, 'e': 19, 't': 20, 'g': 21, 'v': 22, 'R': 23, '\\x0c': 24, 'N': 25, 'f': 26, ':': 27, 'ü': 28, '6': 29, 'j': 30, ',': 31, 'A': 32, '/': 33, 'V': 34, 'h': 35, '“': 36, 'Q': 37, 'L': 38, '7': 39, 'x': 40, 'c': 41, 'b': 42, 'n': 43, 'X': 44, 'a': 45, '4': 46, '–': 47, '‡': 48, 'I': 49, 'G': 50, '2': 51, '-': 52, 'ö': 53, 'K': 54, '†': 55, '[': 56, 'T': 57, 'W': 58, 'Z': 59, '”': 60, 'J': 61, \"'\": 62, '*': 63, '!': 64, ';': 65, '0': 66, 's': 67, 'C': 68, 'd': 69, 'k': 70, 'O': 71, 'u': 72, 'm': 73, 'y': 74, 'r': 75, 'U': 76, 'S': 77, 'M': 78, 'F': 79, '’': 80, 'i': 81, ')': 82, 'B': 83, ' ': 84, '&': 85, '8': 86, '3': 87, 'E': 88, 'H': 89, '1': 90, 'á': 91, '.': 92, 'w': 93}\n",
      "{0: 'q', 1: '(', 2: 'D', 3: 'ô', 4: 'P', 5: 'ó', 6: 'é', 7: '5', 8: '9', 9: 'l', 10: 'Y', 11: 'p', 12: 'o', 13: 'è', 14: 'z', 15: 'ä', 16: ']', 17: '?', 18: '\\n', 19: 'e', 20: 't', 21: 'g', 22: 'v', 23: 'R', 24: '\\x0c', 25: 'N', 26: 'f', 27: ':', 28: 'ü', 29: '6', 30: 'j', 31: ',', 32: 'A', 33: '/', 34: 'V', 35: 'h', 36: '“', 37: 'Q', 38: 'L', 39: '7', 40: 'x', 41: 'c', 42: 'b', 43: 'n', 44: 'X', 45: 'a', 46: '4', 47: '–', 48: '‡', 49: 'I', 50: 'G', 51: '2', 52: '-', 53: 'ö', 54: 'K', 55: '†', 56: '[', 57: 'T', 58: 'W', 59: 'Z', 60: '”', 61: 'J', 62: \"'\", 63: '*', 64: '!', 65: ';', 66: '0', 67: 's', 68: 'C', 69: 'd', 70: 'k', 71: 'O', 72: 'u', 73: 'm', 74: 'y', 75: 'r', 76: 'U', 77: 'S', 78: 'M', 79: 'F', 80: '’', 81: 'i', 82: ')', 83: 'B', 84: ' ', 85: '&', 86: '8', 87: '3', 88: 'E', 89: 'H', 90: '1', 91: 'á', 92: '.', 93: 'w'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print (char_to_ix)\n",
    "print (ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print (vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "  xs, hs, ys, ps, = {}, {}, {}, {} #Empty dicts\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " 078Yéècm;áä[è'éác”S8aqeJ3&él]F Kw‡V\n",
      "&3'HóZ’dQDäüé,Uy?!t'q“ivlôVezPGSryôóSCHôMErvA6&su/–CdQGulRWz,w\f",
      "UlT5HéqdY*!NjgNAHzm9w2‡r*,7oGx?n‡?8Kv]z&jN/NèH.p3\f",
      "px0rYjesiB–Cc;XbF'WeéwrZ5qA'éTcTóT“qGév†áhoa4n.FéY1 \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print ('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [78, 45, 43, 81, 26, 19, 67, 20, 12, 84, 12, 26, 84, 20, 35, 19, 84, 68, 12, 73, 73, 72, 43, 81, 67]\n",
      "targets [45, 43, 81, 26, 19, 67, 20, 12, 84, 12, 26, 84, 20, 35, 19, 84, 68, 12, 73, 73, 72, 43, 81, 67, 20]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print (\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print (\"targets\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 113.582360\n",
      "----\n",
      " u’z’T0Nzéö0zN\f",
      "xj576,’s0P[f43\n",
      "ô9adC,V7vuTöS!;‡h2hXj]:m1öEö3d,\f",
      "“*8–CSfr7qjw\f",
      "emx”GA7äkltykóBt3FGxbV\n",
      "k9è;ie7':n2†Kxf\n",
      "cocHlQèób7SQ‡(ua‡5V.6s\f",
      "JwxtztnätE*Os3cXáJL2fg/ERpGL'Y)RmnWcr*\f",
      "5XvA:Bco1L”]-:oxBq)H\n",
      "(z]D \n",
      "----\n",
      "iter 1000, loss: 89.341639\n",
      "----\n",
      "   aa tit aars feshethe thhnb 0oae t apqnsror otarinlopa  wad  kose coe, thn, iuare rtrfansithiarer, ir!s woay nte rare thases. qsutoernrt itinncur,t sfsces lttoli lif,i rf  gfe naiA ss ieve arrr s *ho \n",
      "----\n",
      "iter 2000, loss: 73.631513\n",
      "----\n",
      " Eess toms cianast i1g iilntionqis th af laes oscawise yu kncae poocIlacoca fettre the chey woeutlose Thes meas\n",
      "gh  fane ads noturtiatroCinlre boal conto pige che saf of tcran, af oltiho pre.ali\n",
      "iyw.ve \n",
      "----\n",
      "iter 3000, loss: 66.055673\n",
      "----\n",
      " wdsl The asment ofmriotlowocihis Fily, nlondotiaantectans wiclionills ringe slerm foritasiy dod as xncal se\n",
      "The coumonine the\n",
      "whe bionui anctsersornt th am gisderiegof theak p, om of Phe mora cacs ofe \n",
      "----\n",
      "iter 4000, loss: 63.384181\n",
      "----\n",
      " d tian on liltle fo wesPys Head cmoprsicedomresitery cfrweagon andiHicil ampeed and nextentintas afuys wise. 8Th  uósydet coremire ny,\n",
      "lee1ceens asis lulhtiis wy sy oof Ior, -Jef lemlatte, arpod,s, be \n",
      "----\n",
      "iter 5000, loss: 59.814298\n",
      "----\n",
      " arad has whopucteitat of\n",
      "es letdexementtondurom the\n",
      "kes that lol ot urn pred of is tWouxdanl ytions lass orisca\n",
      "geest of on rocigas nil the\n",
      "Thatire the to sord mo. that napoundes.n ang hopite oly on g \n",
      "----\n",
      "iter 6000, loss: 59.274810\n",
      "----\n",
      " ry the theon asttorer\n",
      "If I aan ronrad \n",
      "EmGlhe\n",
      "Dewèadaturastorlpath warh.\n",
      "In onXes be bolantreucif Agh of, coprhe ox Gebetspesry ol andre whi pndaal Whe weeling and wurgis o“siocmons themony anrite Lep \n",
      "----\n",
      "iter 7000, loss: 58.223426\n",
      "----\n",
      " 8iv of tint tomer the be ir ane tyate Cofmever of, san izs besimh tharpe of bereased in poresle huopol oftiad diate iftry\n",
      "th nacwy \n",
      "9y boue geergrens sune fungicy tipage prich iked colid of ogitro mea \n",
      "----\n",
      "iter 8000, loss: 58.853046\n",
      "----\n",
      " dedis ‡noto, the sasio aP –ns, le calusgat an Shvey, Cancier therprgelpntimtommuret 1The coveend to it of 1T5salistirte  y whumi?enis tor armente thel itatty af thutr maciss,\n",
      "2l\n",
      "Oxdingeryg an’  [veusm \n",
      "----\n",
      "iter 9000, loss: 56.312877\n",
      "----\n",
      " foeev moppreor avoulind of cageedprigisty iknd the lace by, of the äardarh atorcer cHrettondntes wheree doas, ang ivo coprutise of wargsy bat couss i\f",
      " I thers susoupcextish\n",
      "on ebourgexss wantefron, wh \n",
      "----\n",
      "iter 10000, loss: 55.508365\n",
      "----\n",
      " ry itrg intrey melpsterat theisn axlr.\n",
      "18\n",
      "Be and precous chealo-magmeJ in sapacide beal\n",
      "the [bott –mer wheinqe promme the pory preen ae”t oll oJmandition.\n",
      "*  hitst hard, an, itod ssstois iacl thitpere \n",
      "----\n",
      "iter 11000, loss: 55.630761\n",
      "----\n",
      " ut proun the the Weritatans Ro sturc walaer, boith\n",
      "bn, prrerkach bat\n",
      "is and oretiocis boung of the ffanly,.\n",
      "IBy, in thane an iciouceoind a]s ind wilsich sstaliel in d\n",
      "pomerilgtiniect,\n",
      "Frenor of the of \n",
      "----\n",
      "iter 12000, loss: 55.398880\n",
      "----\n",
      " rare the orl the b: ivardbe adess the touns, w\n",
      "angesterestistde odyssive torein N lofl\n",
      "iper hacees.”\n",
      "boucl thee, the\n",
      "to grom\n",
      "vanh tierevextof sos\n",
      "aseertar itldiet wa punioup uned, 1ôgimed thie son ele \n",
      "----\n",
      "iter 13000, loss: 54.207184\n",
      "----\n",
      " m ondi\n",
      " chusat the prosed qung“yopengeant witf and the thes thatad andis sengh ragiciz lis cemans of” the fovey ats Sin itr Joune\n",
      "Frhater fedconde the proravicgyolesas it whant the tronge vydin; whe p \n",
      "----\n",
      "iter 14000, loss: 55.282536\n",
      "----\n",
      " es, Alis. Copatere Collebath rif sle tal ivoend of ranh blig betall aösm. The in suins of the pralliticion fren larger mads fo thistars, benals hities in\n",
      "lesionis Hnencee tieatier the the if foustarb  \n",
      "----\n",
      "iter 15000, loss: 54.979710\n",
      "----\n",
      " ichl alce the hand ass corproonf the to\n",
      "asit spourty aqvey, inlesfirs in atsenlte why gubomed and, entare of hit s. veviped Gal -of snom at, mentaties,e Conmanleelr-bes. IC an? 11837 “Resion Theantar  \n",
      "----\n",
      "iter 16000, loss: 55.813855\n",
      "----\n",
      " s Covedullor calleutanlim is meding marsser pomints cadut a-witlr alacgars darlherustton\n",
      "urulatiow un atgestergevip of tha in anR, Enesty ans mowminsiecy, the much of tresivalicess ofd lores,\n",
      "ness\n",
      "of, \n",
      "----\n",
      "iter 17000, loss: 53.240020\n",
      "----\n",
      " the istioltias hanissomure to sgal trodeious wire wher Cne3tionto be purtere enanlened, lipsthy seit wiss, of pray lest. of rutititioilh that in turiat fork as where promexitiogrove stomnts prhewe a s \n",
      "----\n",
      "iter 18000, loss: 53.053530\n",
      "----\n",
      " edullliss, comuts; be. The pros the dision2 ditlcon sestion the puliden of whar icn ont want of of witronss stiqure y, inaf pring of wittruncisf formeg operefer tole. Hud-wanin in gavast indl maverit  \n",
      "----\n",
      "iter 19000, loss: 53.573564\n",
      "----\n",
      " Frisuintince Lal anule the As on woure engsed exwaiss bunt of lal( Qepontumande camessat ot they hecier the the follito utul artivery Seite thech fragens restion theind ball senten forp ande op gustin \n",
      "----\n",
      "iter 20000, loss: 53.068040\n",
      "----\n",
      " Corkes to orgerias the bo andrass, capie in whhe dupcoritantiate alm by ar the mourlu comut the by a brite bount, frali , the ofs. In the of\n",
      "more wapdaliese produponitilien, verill therits ormagisepy  \n",
      "----\n",
      "iter 21000, loss: 51.999295\n",
      "----\n",
      " o prughe fices the meancly the\n",
      "mithipsonaturians ot larelt spatare\n",
      "thacas haf sater Noperion ofd sosas ol mediticacy thapty whe et of copmunnonf mes in ancision” ofeps th croplesurl tation exmaty, ind \n",
      "----\n",
      "iter 22000, loss: 53.358817\n",
      "----\n",
      " essotiof mol pos luin as aintwarty geerodueg the dusolesuct\n",
      "thoitenistrouw astran thedugion pongien foritered the bitestan in, polvecere cupotla by monithinge nars tatiats the puress or Perestares lin \n",
      "----\n",
      "iter 23000, loss: 53.759629\n",
      "----\n",
      " ss Forullilly hacies of bes erm of, primese r\n",
      "alleca ous fomnurk prilouprigh us, sad in dercesn worke. Thuroduurok sest, exnd por, by borngiaial unir of noil pomenly.\n",
      "†the seol the lincy atiomceer it  \n",
      "----\n",
      "iter 24000, loss: 53.914685\n",
      "----\n",
      " te fol the bonentint. Watt rec ferted rolay un, mear sula of frast sab)e wave\n",
      "reestief cace it\n",
      "recheld the of heny dene feetysely,  ommy the the of cofus of porkans them\n",
      "to thonf cafucicle puatalss co \n",
      "----\n",
      "iter 25000, loss: 51.525299\n",
      "----\n",
      "  the linal natn of niest lapeed the pedadisas sagn lomlukpsa rich chac. 15s\n",
      "cepid tior and sums.\n",
      "In and\n",
      "crparl phounced fisthiana stash in dels poure, nertonief wists int\n",
      "to gutis To prepol ol of ws c \n",
      "----\n",
      "iter 26000, loss: 51.825330\n",
      "----\n",
      " agesal angrome, sutes peit laluschuroikec\n",
      "Ige e depmopensy,\n",
      "and theaal fer us the melis hill weveristed hive zones retueers aaty thitists of as in, dicfoluvishy to thas lalltety\n",
      "beet rices\n",
      "for bourion \n",
      "----\n",
      "iter 27000, loss: 51.855042\n",
      "----\n",
      " o ysemens, a\n",
      "d.”\n",
      "Drass wis\n",
      "abaser of ay. The preacrily to thinc? Qitoone\n",
      "compuing qung and and withisgre thagnef they soins all wenn,\n",
      "iNw sole of in, tint evern (astionis by casther. No\n",
      "premend in of  \n",
      "----\n",
      "iter 28000, loss: 51.552268\n",
      "----\n",
      " al to toe Anon, the wite vuidren; thesuring and sriger 1842\n",
      "A mend thabeat sociely uercalf.\n",
      "The dimlifityes. Overyyars afobedcnefist” In the edist rest axt and formues finting frmaveristsevred seve be \n",
      "----\n",
      "iter 29000, loss: 50.699093\n",
      "----\n",
      " cy caan foce) chertionduth.\n",
      "In maet\n",
      "als tha thiig the whiro nowieeqrale is is by suct the anerety ceon sig, blcall who\n",
      "valjisty sepoint (an Bansil dictes relitisty an reprode con tiend lat, cisve cont \n",
      "----\n",
      "iter 30000, loss: 52.095746\n",
      "----\n",
      " s ande to ist aag tiars sldster beschrve.\n",
      "Has of labhe recention natimaicry the Coperlal whitatas, andercgenctonagectavizeestift conditiince wea\n",
      "mrit fore to\n",
      "premderdidiminions. “Thatare mastuy henoti \n",
      "----\n",
      "iter 31000, loss: 53.289634\n",
      "----\n",
      " arte? In thasibel, care of Bimer -Enge vaccess Cementturislingcof in memmprenavo Colledclale sovention esatom, as, of din mobl the – sargieut\n",
      "fo\n",
      "poducth and exiuul.\n",
      "2The candrotuin apvo munonss.\n",
      "Qe to \n",
      "----\n",
      "iter 32000, loss: 52.225779\n",
      "----\n",
      " esd\n",
      "of of madeer, the coduoure deae corc infurgien.\n",
      "The lalisted insducte por and exis.d-moresoing” and of itsed of gast thie abe warsas, the far\n",
      "hachor the sivon sotafis\n",
      "beand cov of re to the the la \n",
      "----\n",
      "iter 33000, loss: 50.268543\n",
      "----\n",
      " cseac solems mochions\n",
      "eblisicis the mavuat blitiousich the movound frencue the wor in Vot earomite\n",
      "\n",
      "3The comput Aatise presturt aftist, uis the seoged the bemod incyed\n",
      "\n",
      "Aicte intor bo chested betere c \n",
      "----\n",
      "iter 34000, loss: 50.725092\n",
      "----\n",
      " ert, antiathy, nolusitir they sossly comphemon\n",
      "ous be thatn, shes chetioncionds\n",
      "Gerncokwingolltocinlitites sod iavo or asmetrrusomunerision. The fupsratererto of stociandery cocl theiricgeze thoir the \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 35000, loss: 50.856343\n",
      "----\n",
      " ey exreory lepel ove wat.\n",
      "Qolore belecioch proudiat heplmgeglamn cit Cherise in vaboinl it of apss as. Quper proletionat the warterntion, fisings\n",
      "haation Commey worciar\n",
      "Ity, Modkince hivacty the commu \n",
      "----\n",
      "iter 36000, loss: 50.509375\n",
      "----\n",
      " meat os insted, by the by the dot\n",
      "and on tracha\n",
      "ind and is irdoped this\n",
      "in hat of, of the Wounienlalcis wheoul nedlopllawad the aldistan frolutiand the dingaite fomed of the pormiss\n",
      "by\n",
      "nom; produthodu \n",
      "----\n",
      "iter 37000, loss: 50.646003\n",
      "----\n",
      " Parialmeon All ald eximenatart es\n",
      "aritr, nemment of the in the duny breate o2”,\n",
      "woy in Epos, the dumeisty as faciah ofmaghieny serkear of in feaniath of in cousson a proll promlithed, and It conpeiss  \n",
      "----\n",
      "iter 38000, loss: 51.343119\n",
      "----\n",
      " \n",
      "Commulais arest watf of Compureatiol ruaged the arcalkpuiats usarn Gerionaructy sons por of the utang to org ambunds the Coplls of the Pat te so sequte as and yromtiatue tal al Gat the disalied exbon \n",
      "----\n",
      "iter 39000, loss: 52.994135\n",
      "----\n",
      " peraliplectien Eroveding und the glave brte the cist inned of the, – capecy slat in ils beand. Itte, socian nots\n",
      "furtalpoce erkirimush hehivesiolealget Copbe the\n",
      ", chas of moompisire of prevints omuns \n",
      "----\n",
      "iter 40000, loss: 51.262316\n",
      "----\n",
      " ticns, grehistos vereuindien’s alucomen by avinch in dxants the\n",
      "sucamofir houe soripge vibireaisinge cose, hatered ok, we por, ctenely, sonted retapilurdat by is on, groudiate? It as pragil the anch t \n",
      "----\n",
      "iter 41000, loss: 49.620268\n",
      "----\n",
      " os stat, prepass nalianiath anal saips to in itne. \n",
      "\f",
      "867\n",
      "wore, reav bitar farmokn, wittersifith of the  lionest, wor the spnis ofe the y\f",
      "cher lethers.\n",
      "The lluvo senlicaced preeser the indupro con inal \n",
      "----\n",
      "iter 42000, loss: 50.016612\n",
      "----\n",
      "  alisice, all bustar that inssmay, cosianist reateis worlted\n",
      "deitghireartf ogtents oi mpcedlation the bcciecimars fargeors the premere lagean and, a to mathe and sell on in thin conse the leveersas th \n",
      "----\n",
      "iter 43000, loss: 50.811492\n",
      "----\n",
      " istary, the 1[37. Bincnturet; ford raply bourion and commantiors to the haw wine ingews Computh boss solithe Jess as Past of the\n",
      "the” asted comstioe the bye-communcal whith in ands foinsel disiates th \n",
      "----\n",
      "iter 44000, loss: 49.495304\n",
      "----\n",
      " roussith on the plongherfy nocnciag demong stant on Eters, co tolRst, it an exppil ow to sectay buthuity of comphit* niecteend and frerecomprexian thocy cotio conerthery bources coll ”nich hands of le \n",
      "----\n",
      "iter 45000, loss: 50.582352\n",
      "----\n",
      " d prode chal dertal siveris mouniprlos nouge*stibee hidury who thucees paaty och rourctiedsuares\n",
      "eras reans bewred nas cemin the Gonritied attil the ingreie mand roed mocles sedpralisg a mingllal ware \n",
      "----\n",
      "iter 46000, loss: 51.065442\n",
      "----\n",
      " etefilistry the existor the\n",
      "Chuferver messed in tor in or ;ulals\n",
      "cot.\n",
      "Dlens the when  the wher lrommen-seed of a silay we. Pourusentomestriytiwibeonis, lanis hiboldpibor -themat blifiy rapted in\n",
      "upnd  \n",
      "----\n",
      "iter 47000, loss: 52.588483\n",
      "----\n",
      " eor hivalical bugleffmintiod Gore prostcy inue It it that steont stolly bound\n",
      "will ontinted coducufam and wWo mestimmen hoke fork, –0ursabers inny wasariat the an wanc fequerans “The dishes pach the 2 \n",
      "----\n",
      "iter 48000, loss: 50.170146\n",
      "----\n",
      " nal trragee seppisty, the beubol sol intore the waries uvace\n",
      "duvimeoich no-duchadil the copeniermn woived, iet fucentay the is the stich ar lation chesions are the ontureckial  Ptiflesion harged theic \n",
      "----\n",
      "iter 49000, loss: 49.266438\n",
      "----\n",
      " arl ed\n",
      "i thes by the pind, the comsumopremon exice. Tholutiondins, prolich lal the “\n",
      "hacl the Scobatios, the sody mand in, ild wes, the iade, we it ass in it cond for the realtitiaratusfy. beach whion \n",
      "----\n",
      "iter 50000, loss: 50.060768\n",
      "----\n",
      "  Frite wallait aby wherainn, thearged at the find pranty, expre aveer\n",
      "pheef datutaris buth, the\n",
      "ous printare trait andl coneas, coweuse oftrundat of ragegl “Torstar\n",
      "a the ppolethertrers as and proculu \n",
      "----\n",
      "iter 51000, loss: 49.985139\n",
      "----\n",
      "  arias and ase\n",
      "worke in thighen, in.\n",
      "I1\n",
      "I\n",
      " The pings\n",
      "cuppes asteprey ousy\n",
      "hally fal to and fork, thealy alit Sei]\n",
      "“*th alle a ness ase mends oper frat oplabod crerte 1828\n",
      "“Whider ever bans wilivafe na \n",
      "----\n",
      "iter 52000, loss: 48.986607\n",
      "----\n",
      " d on and pucheucboved the and wrilivia thime the peoprepropecalationgherlonc wivtiads, on thed moue,\n",
      "ratn, and enchinlantion, his and, Blm onde iw wta.\n",
      "The selty it wo the Pasty. The ug preave of cent \n",
      "----\n",
      "iter 53000, loss: 50.431925\n",
      "----\n",
      " somshit fuesert zoll by the beugh reaiget of the\n",
      "-“pow mestrate betwingrestor the restrounat\n",
      "tre. –\n",
      "Futy comrit of\n",
      "shapery, Aliner shallem peneorets ancians, net revortien oc” allore the a3”\n",
      "“With inc \n",
      "----\n",
      "iter 54000, loss: 50.684136\n",
      "----\n",
      " ingpour to prower cletar whionorus bliarl beester abope\n",
      "seaced of somny, the fouret ofthes Itoned of magee wavifal of the reon, pherf abory\n",
      "48 And of to,. The mnle fol hal-.0U proupendetr, dicowisty m \n",
      "----\n",
      "iter 55000, loss: 52.069162\n",
      "----\n",
      " ing of the\n",
      "severom\n",
      "Whittion muted eilit hape and runapplemina1 ubvery bevous harmut that commuxices, Coummand. Wering of kenty and of okirice, anw’ins, the covoledy mave blwe pucly:\n",
      "Aus\n",
      "mill, of the b \n",
      "----\n",
      "iter 56000, loss: 49.054310\n",
      "----\n",
      " ard Sevionation the hish\n",
      "foveancib, covathr, rescwand Andilty, cacats it lans as the gecivat Moritatallend of thipe the pwhenebetllon brusatis ruet thice wheplopes is of tistreseringing profltementtis \n",
      "----\n",
      "iter 57000, loss: 48.977734\n",
      "----\n",
      " as for Affthil cam 9601 siducnes heats anthon catile iager. Zust, the anm ally, the plon. Bite; formsumory apsthigns on lantetss , emustry woch ane.\n",
      "*\n",
      "\n",
      "(heed once of that at”, imunemes on of uss incia \n",
      "----\n",
      "iter 58000, loss: 49.796417\n",
      "----\n",
      "  alls andith corgtal irar, y cary of Mal of command it d, Innd the smours, in co [Drkime, mencict in the fitm pef Fandien leddon of in, ant cisal Condentionss nttan in to th the moun we oret ando ther \n",
      "----\n",
      "iter 59000, loss: 49.273024\n",
      "----\n",
      " er mal unistreress coppiass efeas of br sevejersss theme\n",
      "se. I. The fromitly fulisiolsel, Juless plegralas\n",
      "Forcn cymass commend is sheals, a compo pore we\n",
      "phacesive,\n",
      "1847]\n",
      "raghed\n",
      "lothy renttifios bous \n",
      "----\n",
      "iter 60000, loss: 48.407943\n",
      "----\n",
      "  dictais\n",
      "dereloin onderivies of thot galets is \n",
      "asseas, wist whinut the wo indidonityes their intrade\n",
      "yading noull cas clesitdated\n",
      "poced mance whok indlwes.\n",
      "It sost laco colidl\n",
      "to colmunty the pores a \n",
      "----\n",
      "iter 61000, loss: 50.011795\n",
      "----\n",
      "  wove all it Mammenty, a prourssiontictiotalated ce\n",
      "brelf reop nessors and it the gaciaturaf istrates biw to semanels assice fristyy 1721\n",
      "outbol the itr ass Coment leonh this wine blenret amles whopri \n",
      "----\n",
      "iter 62000, loss: 50.832575\n",
      "----\n",
      "  inqure ofathy of thorpof Berrecing they davave in the babty pasfere thed and, and not propicdoint obane\n",
      "(is acarpjerslavy theal Cocchated eprally they oplis wit\n",
      "coon, aritoud thercheriwal imperetufro \n",
      "----\n",
      "iter 63000, loss: 51.188298\n",
      "----\n",
      " to h. I kny sore, alls\n",
      "mepressbociichery unmer coustr, the moces fentructale” of ind tith serth\n",
      "parstion bouxed and\n",
      "it of and ich on ant on the Compendence a Sot\n",
      "bictower. hished with\n",
      "the to the\n",
      "urode \n",
      "----\n",
      "iter 64000, loss: 48.415376\n",
      "----\n",
      " s his od grongrouges oriction is be the Compeerenist,\n",
      "monese sbon of wheces net of the of prollsor boer-Nalis bound, ondet al “Reportor ceslopage,\n",
      "menisations their the pould be the spide it lecly o\f",
      "  \n",
      "----\n",
      "iter 65000, loss: 48.924878\n",
      "----\n",
      " nses licl the ostalitate (1913 se the poet of to inductry saliatat to souns, osh\n",
      "artion no Gorky, of theelistand the abtre thiaty\n",
      "of bomong of the as the rexis’l, Fracilled, cormen aitictrer, the fims \n",
      "----\n",
      "iter 66000, loss: 48.969982\n",
      "----\n",
      " ocoss, ass”. ]5n, of the Fist min limentco the ansmons attore of ceans with his preplease the turalus indang thiss instrertont promunisuvere for ably whith je prolpulinte by to wass of Commend\n",
      "eexcind \n",
      "----\n",
      "iter 67000, loss: 48.568728\n",
      "----\n",
      " f the ansle a the somenag ofres and end sighiled ase withins, is sup raghe foctoing vilis-suetstion.\n",
      "2y.\n",
      "Brourgeondeliplly soll, [Inder un alwer untreocir, Westint condiciais wien mad the cevevilg lom \n",
      "----\n",
      "iter 68000, loss: 47.980780\n",
      "----\n",
      " ecty, ’n sfoclacar eore formug kisistatio sitty, irtroudy aldisidy of thes prins and ang on Fi-d propedice the yaboish yadllefollitidutiod Mast grilacinacty –ister thes orl, prices ar and\n",
      "cheobrion wa \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 69000, loss: 49.518666\n",
      "----\n",
      " l bang bius ongowip of ivergec ander\n",
      "wxprad the bonly kent of treliryy pressirkund inciall Hand, ceet re)s; encoriblinn of 1agpiach\n",
      "Cownor are at inchens to workentstets wniged on magrion nocl wnol th \n",
      "----\n",
      "iter 70000, loss: 51.067045\n",
      "----\n",
      " ocings Allimnwoie vereny\n",
      "prassaty\n",
      "of to sedrith and wand esantolyre of a\n",
      "exestwor dallompand in theis halith sonnation\n",
      "aruring the. Hon the and the resurtowiens bess the Camas, the salt hast of the\n",
      "la \n",
      "----\n",
      "iter 71000, loss: 50.086975\n",
      "----\n",
      " preitionss, shalistiveom cheores the versition, pork\n",
      "Pulsictasced bydirate wesi, sfacreded thefr.i. 1948\n",
      "\n",
      "Bole wich \n",
      "\f",
      "Commens fork ol to ralition, at of thinisty sul bourgies\n",
      "the hare. Buedsion all mo \n",
      "----\n",
      "iter 72000, loss: 47.917760\n",
      "----\n",
      " n sfadlononies anwerision inction of a\n",
      "purpat ductorimeder bras honalitsed andrciecrien as alp the Heasefd sualisent, yriattion top in ale fous, carpen bugre of\n",
      "the Germen of Fiter itstoniMal boisiing \n",
      "----\n",
      "iter 73000, loss: 48.439423\n",
      "----\n",
      " ethinists wariecialy Manil to producty retogecheif ar Zant of the onate\n",
      "shoy monstich in thanded the och boindis. Thilaty\n",
      "Freftalices, the forking, im acderictiots the Gertis hevinh (22) Worn has by d \n",
      "----\n",
      "iter 74000, loss: 48.449809\n",
      "----\n",
      " atageas of we hers the reparetoriation 6xol and wile, the conwor?\n",
      "Ax thedere the duce we what ratherstasty youn, alggots by way volmand the No the bonce\n",
      "fring wainted prolestust the arist forsss abour \n",
      "----\n",
      "iter 75000, loss: 48.114179\n",
      "----\n",
      " pidestaping nolly.\n",
      "At in Gerswid enbout a for all of pomentor ers of by and to the ivenetry bectict, allithery, of clocina and of by of of\n",
      "angict.\n",
      "Qued cusaul se pafariang in came vilad ald and a cola \n",
      "----\n",
      "iter 76000, loss: 48.136837\n",
      "----\n",
      " ouw of seducistt, indintice ters Chustrkimeaf aver”ed cupemental, tromsticm. a to inculd tourm]\n",
      "es the phemont Chisiatacilf E\f",
      "[Nify bo Nha mergices\n",
      "on sount und ove of wasw.\n",
      "The mote the heclas\n",
      "altiat \n",
      "----\n",
      "iter 77000, loss: 49.199280\n",
      "----\n",
      " comngerh ind the of\n",
      "The comkith\n",
      "wers the prahr enceder vili’te. 22\n",
      "Mod revats ism, 18”\n",
      "Net dughit, whit, lluent sorkery of \n",
      "\f",
      "918, the qusite bruater as by  hage, astarse.\n",
      "The gawer and andwoche all th \n",
      "----\n",
      "iter 78000, loss: 51.030184\n",
      "----\n",
      " ncry Lestuc(to in, in cowal esising “Fruplocis coss pub, un to the the meusing copretents, in son to pow commin thein Se, alult) itry aymunoin, and abered und of\n",
      "Fere the ubits at arias.\n",
      "Cociter\n",
      "custr \n",
      "----\n",
      "iter 79000, loss: 49.408465\n",
      "----\n",
      " y, checerate\n",
      "al strane the\n",
      "lawerald puctionamialf the sind *isurcoy lality, wa co cadave esed linger, no rupeeverfon of of tor), iscromens, in\n",
      "nolls a2t of them” song denert lanced ty the\n",
      "botles the d \n",
      "----\n",
      "iter 80000, loss: 47.646670\n",
      "----\n",
      " tion of in fous; ciste gent, in not on sor ricieret\n",
      "Pary drompecserse soirgh ix\n",
      "lacl ullalitory the wation witter opind fenagrenny ald Coniay in teolico smen\n",
      "hus lamed nachisher whints intomivisath it \n",
      "----\n",
      "iter 81000, loss: 48.185828\n",
      "----\n",
      " f, welrs, in tor\n",
      "haration, vedivas, of vatses thes, sust arsatilet wew in thats\n",
      "butict; to thitien e, edurkces of of thes arat the crerte of prage\n",
      "apredeeutstuce wormker Bugeniaw illtucter wisitickity \n",
      "----\n",
      "iter 82000, loss: 48.740242\n",
      "----\n",
      " harg thachite a cot byenations for in and Commudiend, ropmur: Buclat the font nold the meaus, to that sleatisim pation dast of Snoonstary, a, trucitear, commeny cect and Con the\n",
      "duminaditeas be the.\n",
      "T \n",
      "----\n",
      "iter 83000, loss: 47.343905\n",
      "----\n",
      " pand, ruid co kequs.\n",
      "The wiontonke haciat of colederronberment ordenerise by qurrio aipeed insdevitaile prepwif orlain selememy thes ely]\n",
      "Dugce thed sace bongiincl pronttore\n",
      "ceplepafd deloplalis the q \n",
      "----\n",
      "iter 84000, loss: 48.624619\n",
      "----\n",
      " vevorls a moticiat tho hes ix and odher. Wof geverotty. Theat, the sbcante, te, ngin nonstantsivie-sour “Yeemesting lict asm at, the, fordlent ous mtilures etire abll at bourgions wer pos,\n",
      "we refand i \n",
      "----\n",
      "iter 85000, loss: 49.293692\n",
      "----\n",
      "  uriduly of the lrsed,\n",
      "the gimant witety with of culeine peand Mmevoliaty,\n",
      "to to whue retual al theing and the\n",
      "corssow ats gepisiiging that Con, inlal, thed nagion Paring tem owd inst the latis.\n",
      "Thic  \n",
      "----\n",
      "iter 86000, loss: 51.064860\n",
      "----\n",
      " e\n",
      "Perlicl what the lorry as inturtil\n",
      "rosize, a ceSter priods. 20\n",
      "’rd reacnec\n",
      "aItiony?\n",
      "It the, fintet sos*in\n",
      "arelw obly of  Ennce the russ, whal, ons\n",
      "groll stodlout.\n",
      "\n",
      "Chancencopered a ditint ald tier o \n",
      "----\n",
      "iter 87000, loss: 48.626377\n",
      "----\n",
      " – tho illy, the sfodipitithen to sonus of restucimy Fore whan toistave-Germpeormens, the have thieot whine the Commodedut of ild, the clartiefy ponless of thes, avilatian bonat for that tar – ulmse\n",
      "no \n",
      "----\n",
      "iter 88000, loss: 47.358562\n",
      "----\n",
      " with capcede i1)\n",
      "\n",
      "\f",
      "4 the bourgeous to rebourgioe. In the lake\n",
      "nouryy in the\n",
      "Orom bourgeo and\n",
      "ind phitionm, Proproe trey recesery; bried thie theme alecticlerildhy the conlives, imered les casibo hal n \n",
      "----\n",
      "iter 89000, loss: 48.292684\n",
      "----\n",
      " ropon a wom ix the to sule splors of the ix The the reen to the solisatist icing throo cexpring the Frodemy ad that of mofent stich the Womuncalis, the poonlotiit a\n",
      "pempre. hito the  aI all nevied rus \n",
      "----\n",
      "iter 90000, loss: 48.178204\n",
      "----\n",
      " sts intorich dapmeny. The berud all of the menes, themens indule\n",
      "cefwisn mubouriblistiod by Joul to werghy theio alabliath cond blitory a, drissesss and lat of soln lanle\n",
      "of of malitan ilive preenruty \n",
      "----\n",
      "iter 91000, loss: 47.224733\n",
      "----\n",
      " uist of prietdion the Foruce caplirdiactall dimate. Blle ix and uts; to pringe and.\n",
      "The privel, and to to ctoring one twish wanve forks ese. Mose his, ale it Copporfed the\n",
      "seetad in alfy catil of prep \n",
      "----\n",
      "iter 92000, loss: 48.792859\n",
      "----\n",
      "  pralliat ally the rolutargocis puare clation Defets a puociy\n",
      "be the workes math.\n",
      "\n",
      "the pop thend betoply wis bie restare and of the marted aish othrs.7\n",
      "\n",
      "hedadaf a puting decoud grall veladire Gere the \n",
      "----\n",
      "iter 93000, loss: 49.237680\n",
      "----\n",
      " efs\n",
      "duciat asstaxian Gation with puve cond bing\n",
      "dishabarny, ill cilgienomed corneres, thigh the whinalled\n",
      "that\n",
      "Lonich iveffoiaty, conderent to Frat forsted munutale cace proucs delllitieg of Bagros Co \n",
      "----\n",
      "iter 94000, loss: 50.609865\n",
      "----\n",
      " f ebot fordtiatinish forist, wois rreMance, the sin grboy oTly in\n",
      "langed molily the fraricar, the Engraind checent it nandrecW of the dised at \n",
      "Phant, thisie, ceatiblise purtenced it of uss aftheries  \n",
      "----\n",
      "iter 95000, loss: 47.528009\n",
      "----\n",
      "  cher’at what soobost but of the the b– than stattite fady thesed atanisterson theious\n",
      "it toom anter ofstarisil by iveriectiolmint ip puritirafias in solfincas, the Gerceclires\n",
      "aittacy:\n",
      "Bo rotirtewouv \n",
      "----\n",
      "iter 96000, loss: 47.449760\n",
      "----\n",
      " e heriey be fletereny. There condadis crolists of cowericted ured, Pars everly in of to the boustifionical\n",
      "complodrsore, witications.\n",
      "Orinfe a6 in in wer of nodion†t tion. If chome as puts of cutone C \n",
      "----\n",
      "iter 97000, loss: 48.281048\n",
      "----\n",
      " migh intar.\n",
      "No0; the coluciaty in istoriany in thef an to a Intich the moull.\n",
      "Thish hepepstrer sere redisir aboblial all depisostponccarmen thockich in\n",
      "nluteas to etre, the prouch habery; Befsan agll  \n",
      "----\n",
      "iter 98000, loss: 47.713793\n",
      "----\n",
      " m the preit Bucuely proing neste\n",
      "vevolacdare and bourde\n",
      "oration trees cle. The Cord las the wor the warpuoracy. Propertions a wholy lass tho lesters anl jof laverytoe onging home yCorginiSg suct and,  \n",
      "----\n",
      "iter 99000, loss: 46.919203\n",
      "----\n",
      " s of masly dicival bin pucpef to bes, worked ase ias inwory”, the the sadition, its, the wistral is reitiel proborge devoditial of asts\n",
      "a\n",
      "prolit there, and it al5, reston of the Donic pithir on a bric \n",
      "----\n",
      "iter 100000, loss: 48.587392\n",
      "----\n",
      " ans untro dorute, pwodociats al Frelicwagh prallf antaitopef the war inhivistaccat.\n",
      "The mon) dafunesion that reatmorstion lakilk on Polien of ancersalistts, proe hanl Guaper: The fore Less, by the\n",
      "sit \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print ('iter %d, loss: %f' % (n, smooth_loss) )# print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
